import mysql.connector
import numpy as np
import pandas as pd
import random
import datetime
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt




class CryptoData(object):
    def __init__(self,
                 stock_sym,
                 fsym,
                 tsym,
                 input_size=1,
                 num_steps=30,
                 test_ratio=0.1,
                 normalized=True):
        self.stock_sym = stock_sym
        self.fsym = fsym
        self.tsym = tsym
        self.input_size = input_size
        self.num_steps = num_steps
        self.test_ratio = test_ratio
        self.normalized = normalized



        connection = mysql.connector.connect(user='root', database='CryptoCompare', password='Greenacres29')
        cursor = connection.cursor()

        # Get Data
        query = "SELECT * FROM storage where fsym=%s and tsym=%s order by time asc"
        cursor.execute(query, (self.fsym, self.tsym))
        rows = cursor.fetchall()
        raw_seq = ()
        dates = ()
        i=0
        for row in rows:
            if i < 2800:
                row = list(row)
                row.pop(0)
                dates = (row[0],) + dates
                row[0] = datetime.datetime.combine(row[0].date(), row[0].time()).timestamp()
                row.pop(5)
                row.pop(5)
                row.pop(5)
                row.pop(0)
                row = tuple(row)
                raw_seq = (row,) + raw_seq
                i = i + 1
            else:
                break



        raw_seq = np.array(raw_seq)
        raw_seq = feature_normalize(raw_seq)
        num = len(raw_seq)
        train_size = int(num * 0.9)
        dates = list(dates)
        dates = dates[train_size + 2:len(raw_seq) ]
        self.dates = np.array(dates)
        self.train_X, self.train_y, self.test_X, self.test_y = self.prepare_data(raw_seq,num,train_size)
        '''a = [[ 0.06695746],
 [ 0.06695609],
 [ 0.06695711],
 [ 0.06695604],
 [ 0.06695458],
 [ 0.06695323],
 [ 0.06695274],
 [ 0.06694894],
 [ 0.06694715],
 [ 0.06694385],
 [ 0.06693934],
 [ 0.06693677],
 [ 0.06693674],
 [ 0.06693809],
 [ 0.06693795],
 [ 0.06693701],
 [ 0.06693989],
 [ 0.06694238],
 [ 0.06693756],
 [ 0.06693783],
 [ 0.06693763],
 [ 0.06693846],
 [ 0.06693856],
 [ 0.06693894],
 [ 0.06694136],
 [ 0.06694116],
 [ 0.06694293],
 [ 0.06694235],
 [ 0.06694319],
 [ 0.06694216],
 [ 0.0669402 ],
 [ 0.06693695],
 [ 0.0669373 ],
 [ 0.06693957],
 [ 0.06694409],
 [ 0.06694189],
 [ 0.06694227],
 [ 0.06694214],
 [ 0.06694271],
 [ 0.06694324],
 [ 0.06694396],
 [ 0.06694447],
 [ 0.06694586],
 [ 0.0669415 ],
 [ 0.06694245],
 [ 0.06694417],
 [ 0.06694636],
 [ 0.06694681],
 [ 0.06694416],
 [ 0.06694515],
 [ 0.06694484],
 [ 0.06694652],
 [ 0.06694353],
 [ 0.06694513],
 [ 0.06694628],
 [ 0.06694702],
 [ 0.06694888],
 [ 0.06694448],
 [ 0.06694447],
 [ 0.06694125],
 [ 0.06694567],
 [ 0.06694903],
 [ 0.06695385],
 [ 0.06694964],
 [ 0.06694997],
 [ 0.06695123],
 [ 0.06695124],
 [ 0.06695144],
 [ 0.0669535 ],
 [ 0.0669546 ],
 [ 0.06695868],
 [ 0.06695852],
 [ 0.0669448 ],
 [ 0.06694473],
 [ 0.066946  ],
 [ 0.06694566],
 [ 0.06694422],
 [ 0.06693701],
 [ 0.06693638],
 [ 0.06694259],
 [ 0.06694083],
 [ 0.06692461],
 [ 0.06692272],
 [ 0.06692294],
 [ 0.06692268],
 [ 0.06692217],
 [ 0.06692183],
 [ 0.06692389],
 [ 0.0669232 ],
 [ 0.06692409],
 [ 0.06692347],
 [ 0.06692185],
 [ 0.06691685],
 [ 0.06691867],
 [ 0.06692136],
 [ 0.06692021],
 [ 0.06692047],
 [ 0.06692395],
 [ 0.06691958],
 [ 0.06691818],
 [ 0.06691753],
 [ 0.06691755],
 [ 0.06691703],
 [ 0.06691752],
 [ 0.06691746],
 [ 0.06691629],
 [ 0.06691633],
 [ 0.06691631],
 [ 0.06691623],
 [ 0.06691647],
 [ 0.06691632],
 [ 0.06691632],
 [ 0.06691636],
 [ 0.0669169 ],
 [ 0.0669157 ],
 [ 0.0669152 ],
 [ 0.06691509],
 [ 0.06691387],
 [ 0.06691403],
 [ 0.06691419],
 [ 0.06691387],
 [ 0.06691471],
 [ 0.06691545],
 [ 0.06691372],
 [ 0.06691368],
 [ 0.06691387],
 [ 0.0669149 ],
 [ 0.06691369],
 [ 0.06691488],
 [ 0.06691355],
 [ 0.06691339],
 [ 0.06691395],
 [ 0.06691259],
 [ 0.06691452],
 [ 0.06691402],
 [ 0.06691559],
 [ 0.06691302],
 [ 0.0669129 ],
 [ 0.06691433],
 [ 0.06691438],
 [ 0.06691507],
 [ 0.06691328],
 [ 0.06691319],
 [ 0.06691578],
 [ 0.06691536],
 [ 0.06691533],
 [ 0.06691571],
 [ 0.06691535],
 [ 0.06691551],
 [ 0.06691563],
 [ 0.06691603],
 [ 0.06691526],
 [ 0.06691602],
 [ 0.06691539],
 [ 0.06691627],
 [ 0.0669155 ],
 [ 0.06691507],
 [ 0.06691526],
 [ 0.06691601],
 [ 0.06691591],
 [ 0.06691636],
 [ 0.06691396],
 [ 0.06691688],
 [ 0.0669167 ],
 [ 0.06691987],
 [ 0.06692494],
 [ 0.06692737],
 [ 0.0669159 ],
 [ 0.06691396],
 [ 0.0669167 ],
 [ 0.06691157],
 [ 0.06691232],
 [ 0.06691244],
 [ 0.06691188],
 [ 0.06691156],
 [ 0.06691126],
 [ 0.06691201],
 [ 0.06691079],
 [ 0.06691192],
 [ 0.06691044],
 [ 0.06690757],
 [ 0.06690764],
 [ 0.06690789],
 [ 0.0669075 ],
 [ 0.06690736],
 [ 0.0669076 ],
 [ 0.06690749],
 [ 0.06690747],
 [ 0.06690839],
 [ 0.06690899],
 [ 0.06690782],
 [ 0.06690767],
 [ 0.06690747],
 [ 0.06690879],
 [ 0.06690875],
 [ 0.06690946],
 [ 0.06690589],
 [ 0.06690568],
 [ 0.06690562],
 [ 0.06690561],
 [ 0.06690556],
 [ 0.06690559],
 [ 0.0669056 ],
 [ 0.0669056 ],
 [ 0.0669056 ],
 [ 0.0669056 ],
 [ 0.06690559],
 [ 0.06690559],
 [ 0.06690559],
 [ 0.0669056 ],
 [ 0.06690565],
 [ 0.06690562],
 [ 0.0669056 ],
 [ 0.06690565],
 [ 0.06690571],
 [ 0.06690565],
 [ 0.06690557],
 [ 0.06690556],
 [ 0.06690561],
 [ 0.06690928],
 [ 0.06690568],
 [ 0.06690558],
 [ 0.06690571],
 [ 0.06690561],
 [ 0.06690559],
 [ 0.0669056 ],
 [ 0.06690562],
 [ 0.0669056 ],
 [ 0.06690572],
 [ 0.06690563],
 [ 0.06690565],
 [ 0.06690577],
 [ 0.06690571],
 [ 0.06690572],
 [ 0.06690718],
 [ 0.06690571],
 [ 0.06690574],
 [ 0.06690577],
 [ 0.06690572],
 [ 0.06690581],
 [ 0.06690578],
 [ 0.06690586],
 [ 0.06690598],
 [ 0.06690586],
 [ 0.06690583],
 [ 0.06690584],
 [ 0.06690609],
 [ 0.06690638],
 [ 0.06690598],
 [ 0.06690583],
 [ 0.06690592],
 [ 0.06690589],
 [ 0.06690624],
 [ 0.06690641],
 [ 0.06690611],
 [ 0.06690636],
 [ 0.06690559],
 [ 0.06690563],
 [ 0.06690566],
 [ 0.0669056 ],
 [ 0.0669056 ],
 [ 0.06690571],
 [ 0.06690562],
 [ 0.06690581],
 [ 0.06690625],
 [ 0.06690614],
 [ 0.06690627],
 [ 0.06690576],
 [ 0.06690575],
 [ 0.06690545],
 [ 0.06690545],
 [ 0.06690577],
 [ 0.06690606],
 [ 0.06690659],
 [ 0.06690674],
 [ 0.06690649],
 [ 0.066907  ],
 [ 0.06690707]]

        #b =[-0.39667985,-0.39665651,-0.39667207,-0.39669152,-0.39671486,-0.39673823,-0.3967343,-0.39676445,-0.39678876,-0.39682376,-0.39683664,-0.39683932,-0.39683154,-0.39683007,-0.39683543,-0.3968471,-0.39685877,-0.39682026,-0.3968191,-0.39682217,-0.39681809,-0.3968161,-0.39681501,-0.39681248,-0.39680432,-0.39679063,-0.3967795,-0.39678607,-0.39679276,-0.3968086,-0.39682804,-0.39683501,-0.39682571,-0.39680568,-0.39680222,-0.39678876,-0.39678487,-0.39677518,-0.39677612,-0.3967662,-0.39678098,-0.3967604,-0.39678701,-0.39678487,-0.39677903,-0.39677324,-0.39676919,-0.39677281,-0.39675799,-0.39675768,-0.39676534,-0.39678876,-0.39677709,-0.39675064,-0.39676888,-0.39673532,-0.3967732,-0.39678479,-0.39679868,-0.39679265,-0.39675414,-0.39677363,-0.39671875,-0.39671486,-0.39671486,-0.39670708,-0.39671486,-0.39670319,-0.39670708,-0.39674197,-0.3966993,-0.3967662,-0.39677709,-0.39677332,-0.39676542,-0.39680782,-0.39685488,-0.39684477,-0.39685099,-0.396921,-0.3969366,-0.39695251,-0.39694979,-0.39695943,-0.39696107,-0.39696379,-0.39695994,-0.39695119,-0.39695045,-0.39695994,-0.39697157,-0.39700152,-0.39700152,-0.39698732,-0.39697281,-0.39697312,-0.39696768,-0.39699973,-0.39699926,-0.39699623,-0.39699623,-0.39699763,-0.39699767,-0.3969988,-0.39700735,-0.39700696,-0.397007,-0.39700852,-0.39700657,-0.39700657,-0.39700657,-0.39700657,-0.39700657,-0.39701396,-0.39702019,-0.39702019,-0.39702606,-0.3970268,-0.39702602,-0.39702602,-0.39702991,-0.39701941,-0.39702987,-0.39702952,-0.39702991,-0.39702602,-0.39703069,-0.39702731,-0.39703384,-0.39703769,-0.39703458,-0.39704391,-0.39704547,-0.39703038,-0.39703263,-0.39704391,-0.39704936,-0.39704353,-0.39702563,-0.39702408,-0.39703477,-0.39704228,-0.39703384,-0.39701824,-0.39701319,-0.39701264,-0.39701435,-0.39701319,-0.39701323,-0.39701128,-0.39701564,-0.39701319,-0.39701435,-0.3970191,-0.39703384,-0.39703652,-0.39701894,-0.39701474,-0.39701591,-0.39701894,-0.39703649,-0.39702991,-0.39704158,-0.39702875,-0.39699102,-0.39697157,-0.39702213,-0.3970338,-0.39704815,-0.39704788,-0.39704722,-0.39704839,-0.3970459,-0.39704936,-0.39705593,-0.39705025,-0.3970648,-0.39707192,-0.39707853,-0.39708223,-0.39708339,-0.39708164,-0.39708476,-0.39708553,-0.39708343,-0.39708359,-0.39708398,-0.39708242,-0.39708359,-0.39708242,-0.39708635,-0.39708631,-0.39708573,-0.39708678,-0.39708948,-0.3970972,-0.39709883,-0.39709938,-0.39709942,-0.39709949,-0.39709938,-0.39709916,-0.39709919,-0.39709918,-0.39709919,-0.39709907,-0.39709915,-0.39709914,-0.39709907,-0.39709903,-0.39709907,-0.39709889,-0.39709911,-0.39709887,-0.39709954,-0.39710031,-0.39709919,-0.39709977,-0.39709915,-0.39709907,-0.39709934,-0.3970985,-0.39709922,-0.39709949,-0.39709915,-0.39709954,-0.3970993,-0.3970993,-0.397099,,-0.39709959,-0.3970986,-0.3970988,-0.39709992,-0.39709799,-0.39709837,-0.39709833,-0.39709798,-0.39709837,-0.39709806,-0.39709798,-0.39709801,-0.39709759,-0.39709744,-0.39709779,-0.39709732,-0.39709681,-0.39709603,-0.39709779,-0.39709787,-0.3970972,-0.39709817,-0.39709603,-0.3970972,-0.39709603,-0.39709565,-0.39709957,-0.39710031,-0.39709903,-0.39709954,-0.39710109,-0.39709992,-0.39709992,-0.3970995,-0.39709687,-0.39709887,-0.39709607,-0.39710035,-0.39709992,-0.39710148,-0.39710362,-0.39710205,-0.39709891,-0.39710362,-0.39709245,-0.39709419,-0.39709183,-0.39708987,-0.397104]
        print(self.test_y.shape)
        print(self.dates.shape)
        plt.figure(figsize=(100,50))
        plt.plot(dates, self.test_y, label='truth')
        #plt.plot(dates, a, label='prediction')
        plt.xlabel("day")
        plt.ylabel("normalized price")
        plt.show()'''


    def prepare_data(self,seq,num,train_size):
        X_train = seq[0:train_size]
        X_train = X_train.reshape(train_size,1,4)
        X_train = np.array(X_train)
        y_train = [row[3] for row in seq[1:train_size+1]]
        y_train = np.array(y_train)
        X_test = seq[train_size+1:len(seq)-1]
        X_test = X_test.reshape(len(X_test),1,4)
        X_test = np.array(X_test)
        y_test = [row[3] for row in seq[train_size+2:len(seq)]]
        y_test = np.array(y_test)
        print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)





        return X_train, y_train, X_test, y_test





    def generate_one_epoch(self, batch_size):
        num_batches = int(len(self.train_X)) // batch_size
        if batch_size * num_batches < len(self.train_X):
            num_batches += 1
        batch_indices = list(range(num_batches))
        random.shuffle(batch_indices)
        for j in batch_indices:
            batch_X = self.train_X[j * batch_size: (j + 1) * batch_size]
            batch_y = self.train_y[j * batch_size: (j + 1) * batch_size]
            #assert set(map(len, batch_X)) == {self.num_steps}
            yield batch_X, batch_y

def feature_normalize(dataset):
    mu = np.mean(dataset,axis=0)
    sigma = np.std(dataset,axis=0)
    return (dataset - mu)/sigma

def append_bias_reshape(features,labels):
    n_training_samples = features.shape[0]
    n_dim = features.shape[1]
    f = np.reshape(np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim + 1])
    l = np.reshape(labels,[n_training_samples,1])
    return f, l


'''
def useless:
    for row in seq[0:train_size - 1]:
        row = row.reshape(30, row.shape[0])
        X_train = (row,) + X_train
    X_train = np.array(X_train)
    X_test = ()
    for row in seq[train_size:len(seq) - 1]:
        row = row.reshape(30, 1, row.shape[0])
        X_test = (row,) + X_test
    X_test = np.array(X_test)
    y_train = [row[3] for row in seq[1:train_size]]
    y_train = np.array(y_train)
    y_test = [row[3] for row in seq[train_size + 1:num]]
    y_test = np.array(y_test)
    print(X_train.shape)
    
    
    
    #seq = [np.array(seq[i * self.input_size: (i + 1) * self.input_size])
               #for i in range(len(seq) // self.input_size)]
        #seq = [seq[0] / seq[0][0] - 1.0] + [
         #   curr / seq[i][-1] - 1.0 for i, curr in enumerate(seq[1:])]

        # split into groups of num_steps
        #X = np.array([seq[i: i + self.num_steps] for i in range(len(seq) - self.num_steps)])
        #y = np.array([seq[i + self.num_steps] for i in range(len(seq) - self.num_steps)])
        
        '''